// Properties for stochastic MDP with hazard

// P1: Maximum probability of reaching goal
Pmax=? [ F goal ]

// P2: Maximum probability of reaching goal while staying safe
Pmax=? [ safe U goal ]

// P3: Minimum probability of hitting hazard
Pmin=? [ F hazard ]

// P4: Expected steps to reach goal (optimal policy)
R{"steps"}min=? [ F (goal | hazard) ]

// P5: Maximum expected total reward
R{"total"}max=? [ F (goal | hazard) ]

// P6: Can reach goal with prob >= 0.9?
Pmax>=0.9 [ F goal ]

// P7: Probability of reaching goal within 10 steps
Pmax=? [ F<=10 goal ]

